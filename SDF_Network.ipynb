{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84edfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22066220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlene/anaconda3/envs/SDF/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/charlene/anaconda3/envs/SDF/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default resnet\n"
     ]
    }
   ],
   "source": [
    "# Define encoder \n",
    "# First load resnet\n",
    "model_conv = models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    #Requires_grad = True if is needed to be computed for this Tensor. All are True by default\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 256)    #Add a new last fully connected layer that will output a 256-features vector\n",
    "#model_conv.cuda()                          #Allow the model to run on CUDA\n",
    "#torch.cuda.set_device(device)\n",
    "\n",
    "#Encoder is the sequential model of a 2D convolutional model + the modified resnet50 model\n",
    "encoder = nn.Sequential(\n",
    "    #nn.Conv2d(in_channels=6,out_channels=3,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "    nn.Conv2d(6,3,7,2,3,bias=False),\n",
    "    model_conv)\n",
    "\n",
    "print(\"default resnet\")\n",
    "#print(model_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce531b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our resnet\n"
     ]
    }
   ],
   "source": [
    "print(\"our resnet\")\n",
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2647f4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Sequential(\n",
      "  (0): Conv2d(6, 3, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(encoder.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce34c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decoder\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = [\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ReLU(),\n",
    "        ]\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        #Add the last layer that will output from 512 to 1 feature\n",
    "        self.layers.append(nn.Linear(self.hidden_size, self.output_size))\n",
    "\n",
    "        #Compile all the layers into a sequential model\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    #Computation performed at every call\n",
    "    def forward(self, x):\n",
    "        sdf_xyz = torch.tanh(self.layers(x))\n",
    "        return sdf_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fbd68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input_size=259 (256 features + 3 positions), hidden_size=512?, output_size=1 (sdf?), num_layers=8\n",
    "decoder = FFNN(259, 512, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=259, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0def16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/render0/img/03001627/ffd258571807e6425b1205fcf56bb774', 'dataset/render0/img/03001627/ffd3064cff5757695ecd29875b6f0d44', 'dataset/render0/img/03001627/ffd616229a97642c7ea8c9f2db0a45da', 'dataset/render0/img/03001627/ffd9387a533fe59e251990397636975f', 'dataset/render0/img/03001627/ffdc46ab1cfe759ce6fe3612af521500', 'dataset/render0/img/03001627/ffed7e95160f8edcdea0b1aceafe4876', 'dataset/render0/img/03001627/fff29a99be0df71455a52e01ade8eb6a', 'dataset/render0/img/03001627/fffda9f09223a21118ff2740a556cc3', 'dataset/render0/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee', 'dataset/render0/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6', 'dataset/render0/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647', 'dataset/render0/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83', 'dataset/render0/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a', 'dataset/render0/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40', 'dataset/render0/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34', 'dataset/render0/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15', 'dataset/render1/img/03001627/ffd258571807e6425b1205fcf56bb774', 'dataset/render1/img/03001627/ffd3064cff5757695ecd29875b6f0d44', 'dataset/render1/img/03001627/ffd616229a97642c7ea8c9f2db0a45da', 'dataset/render1/img/03001627/ffd9387a533fe59e251990397636975f', 'dataset/render1/img/03001627/ffdc46ab1cfe759ce6fe3612af521500', 'dataset/render1/img/03001627/ffed7e95160f8edcdea0b1aceafe4876', 'dataset/render1/img/03001627/fff29a99be0df71455a52e01ade8eb6a', 'dataset/render1/img/03001627/fffda9f09223a21118ff2740a556cc3', 'dataset/render1/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee', 'dataset/render1/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6', 'dataset/render1/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647', 'dataset/render1/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83', 'dataset/render1/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a', 'dataset/render1/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40', 'dataset/render1/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34', 'dataset/render1/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15', 'dataset/render2/img/03001627/ffd258571807e6425b1205fcf56bb774', 'dataset/render2/img/03001627/ffd3064cff5757695ecd29875b6f0d44', 'dataset/render2/img/03001627/ffd616229a97642c7ea8c9f2db0a45da', 'dataset/render2/img/03001627/ffd9387a533fe59e251990397636975f', 'dataset/render2/img/03001627/ffdc46ab1cfe759ce6fe3612af521500', 'dataset/render2/img/03001627/ffed7e95160f8edcdea0b1aceafe4876', 'dataset/render2/img/03001627/fff29a99be0df71455a52e01ade8eb6a', 'dataset/render2/img/03001627/fffda9f09223a21118ff2740a556cc3', 'dataset/render2/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee', 'dataset/render2/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6', 'dataset/render2/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647', 'dataset/render2/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83', 'dataset/render2/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a', 'dataset/render2/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40', 'dataset/render2/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34', 'dataset/render2/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15', 'dataset/render3/img/03001627/ffd258571807e6425b1205fcf56bb774', 'dataset/render3/img/03001627/ffd3064cff5757695ecd29875b6f0d44', 'dataset/render3/img/03001627/ffd616229a97642c7ea8c9f2db0a45da', 'dataset/render3/img/03001627/ffd9387a533fe59e251990397636975f', 'dataset/render3/img/03001627/ffdc46ab1cfe759ce6fe3612af521500', 'dataset/render3/img/03001627/ffed7e95160f8edcdea0b1aceafe4876', 'dataset/render3/img/03001627/fff29a99be0df71455a52e01ade8eb6a', 'dataset/render3/img/03001627/fffda9f09223a21118ff2740a556cc3', 'dataset/render3/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee', 'dataset/render3/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6', 'dataset/render3/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647', 'dataset/render3/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83', 'dataset/render3/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a', 'dataset/render3/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40', 'dataset/render3/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34', 'dataset/render3/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15', 'dataset/render4/img/03001627/ffd258571807e6425b1205fcf56bb774', 'dataset/render4/img/03001627/ffd3064cff5757695ecd29875b6f0d44', 'dataset/render4/img/03001627/ffd616229a97642c7ea8c9f2db0a45da', 'dataset/render4/img/03001627/ffd9387a533fe59e251990397636975f', 'dataset/render4/img/03001627/ffdc46ab1cfe759ce6fe3612af521500', 'dataset/render4/img/03001627/ffed7e95160f8edcdea0b1aceafe4876', 'dataset/render4/img/03001627/fff29a99be0df71455a52e01ade8eb6a', 'dataset/render4/img/03001627/fffda9f09223a21118ff2740a556cc3', 'dataset/render4/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee', 'dataset/render4/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6', 'dataset/render4/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647', 'dataset/render4/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83', 'dataset/render4/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a', 'dataset/render4/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40', 'dataset/render4/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34', 'dataset/render4/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15']\n",
      "['sdf/ffd258571807e6425b1205fcf56bb774', 'sdf/ffd3064cff5757695ecd29875b6f0d44', 'sdf/ffd616229a97642c7ea8c9f2db0a45da', 'sdf/ffd9387a533fe59e251990397636975f', 'sdf/ffdc46ab1cfe759ce6fe3612af521500', 'sdf/ffed7e95160f8edcdea0b1aceafe4876', 'sdf/fff29a99be0df71455a52e01ade8eb6a', 'sdf/fffda9f09223a21118ff2740a556cc3', 'sdf/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee', 'sdf/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6', 'sdf/u481ebf18-4bbb-4b49-90c9-7a1e9348b647', 'sdf/u6028f63e-4111-4412-9098-fe5f4f0c7c83', 'sdf/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a', 'sdf/uca24feec-f0c0-454c-baaf-561530686f40', 'sdf/udf068a6b-e65b-430b-bc17-611b062e2e34', 'sdf/ue639c33f-d415-458c-8ff8-2ef68135af15']\n"
     ]
    }
   ],
   "source": [
    "#Load data for training\n",
    "import os\n",
    "\n",
    "all_datapath_img = [] # with path = ./obj_id/image_subdiretory....\n",
    "all_sdfpath_obj = [] # with : one_sdf_path = ./obj_id.npy (or obj_id.txt)\n",
    "\n",
    "#Load data for images\n",
    "dataset_folder = \"dataset\"\n",
    "\n",
    "file_list = sorted(os.listdir(dataset_folder))\n",
    "\n",
    "for i, file in enumerate(file_list):    \n",
    "    file_img = sorted(os.listdir(os.path.join(dataset_folder, file, \"img/03001627\")))\n",
    "    \n",
    "    for idx, imgFile in enumerate(file_img):\n",
    "        datapath_img = os.path.join(dataset_folder, file, \"img/03001627\", imgFile)\n",
    "        all_datapath_img.append(datapath_img)\n",
    "        #print(idx, \" : \", datapath_img)\n",
    "        \n",
    "print(all_datapath_img)\n",
    "\n",
    "#Load data for points array and their sdf value\n",
    "dataset_folder = \"sdf\"\n",
    "\n",
    "file_list = sorted(os.listdir(dataset_folder))\n",
    "for idx, file in enumerate(file_list):\n",
    "    sdfpath_obj = os.path.join(dataset_folder, file)\n",
    "    all_sdfpath_obj.append(sdfpath_obj)\n",
    "    \n",
    "print(all_sdfpath_obj)\n",
    "\n",
    "# Dictionary that maps integer to its string value \n",
    "label_dict = {}\n",
    "\n",
    "# List to store integer labels \n",
    "int_labels = []\n",
    "\n",
    "for i in range(len(all_datapath_img)):\n",
    "    label_dict[i] = all_datapath_img[i]\n",
    "    int_labels.append(i)\n",
    "\n",
    "#print(label_dict)\n",
    "\n",
    "# obj.txt or obj.npy should contain all point (xyz) and their sdf for one obj object\n",
    "# EX : all_datapath_img = [\"./03001627/1a6f615e8b1b5ae4dbbc9440457e303e(obj_id)/subdirectory_1\", \"./03001627/1a6f615e8b1b5ae4dbbc9440457e303e(obj_id)/subdirectory_2\", ...]\n",
    "# EX : all_sdfpath_obj = [\".../1a6f615e8b1b5ae4dbbc9440457e303e.npy\", \".../1a8bbf2994788e2743e99e0cae970928.npy\", etc.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28d7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(int_labels))\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06af29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)  :  dataset/render0/img/03001627/ffd258571807e6425b1205fcf56bb774\n",
      "tensor(1)  :  dataset/render0/img/03001627/ffd3064cff5757695ecd29875b6f0d44\n",
      "tensor(2)  :  dataset/render0/img/03001627/ffd616229a97642c7ea8c9f2db0a45da\n",
      "tensor(3)  :  dataset/render0/img/03001627/ffd9387a533fe59e251990397636975f\n",
      "tensor(4)  :  dataset/render0/img/03001627/ffdc46ab1cfe759ce6fe3612af521500\n",
      "tensor(5)  :  dataset/render0/img/03001627/ffed7e95160f8edcdea0b1aceafe4876\n",
      "tensor(6)  :  dataset/render0/img/03001627/fff29a99be0df71455a52e01ade8eb6a\n",
      "tensor(7)  :  dataset/render0/img/03001627/fffda9f09223a21118ff2740a556cc3\n",
      "tensor(8)  :  dataset/render0/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee\n",
      "tensor(9)  :  dataset/render0/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6\n",
      "tensor(10)  :  dataset/render0/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647\n",
      "tensor(11)  :  dataset/render0/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83\n",
      "tensor(12)  :  dataset/render0/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a\n",
      "tensor(13)  :  dataset/render0/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40\n",
      "tensor(14)  :  dataset/render0/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34\n",
      "tensor(15)  :  dataset/render0/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15\n",
      "tensor(16)  :  dataset/render1/img/03001627/ffd258571807e6425b1205fcf56bb774\n",
      "tensor(17)  :  dataset/render1/img/03001627/ffd3064cff5757695ecd29875b6f0d44\n",
      "tensor(18)  :  dataset/render1/img/03001627/ffd616229a97642c7ea8c9f2db0a45da\n",
      "tensor(19)  :  dataset/render1/img/03001627/ffd9387a533fe59e251990397636975f\n",
      "tensor(20)  :  dataset/render1/img/03001627/ffdc46ab1cfe759ce6fe3612af521500\n",
      "tensor(21)  :  dataset/render1/img/03001627/ffed7e95160f8edcdea0b1aceafe4876\n",
      "tensor(22)  :  dataset/render1/img/03001627/fff29a99be0df71455a52e01ade8eb6a\n",
      "tensor(23)  :  dataset/render1/img/03001627/fffda9f09223a21118ff2740a556cc3\n",
      "tensor(24)  :  dataset/render1/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee\n",
      "tensor(25)  :  dataset/render1/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6\n",
      "tensor(26)  :  dataset/render1/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647\n",
      "tensor(27)  :  dataset/render1/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83\n",
      "tensor(28)  :  dataset/render1/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a\n",
      "tensor(29)  :  dataset/render1/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40\n",
      "tensor(30)  :  dataset/render1/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34\n",
      "tensor(31)  :  dataset/render1/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15\n",
      "tensor(32)  :  dataset/render2/img/03001627/ffd258571807e6425b1205fcf56bb774\n",
      "tensor(33)  :  dataset/render2/img/03001627/ffd3064cff5757695ecd29875b6f0d44\n",
      "tensor(34)  :  dataset/render2/img/03001627/ffd616229a97642c7ea8c9f2db0a45da\n",
      "tensor(35)  :  dataset/render2/img/03001627/ffd9387a533fe59e251990397636975f\n",
      "tensor(36)  :  dataset/render2/img/03001627/ffdc46ab1cfe759ce6fe3612af521500\n",
      "tensor(37)  :  dataset/render2/img/03001627/ffed7e95160f8edcdea0b1aceafe4876\n",
      "tensor(38)  :  dataset/render2/img/03001627/fff29a99be0df71455a52e01ade8eb6a\n",
      "tensor(39)  :  dataset/render2/img/03001627/fffda9f09223a21118ff2740a556cc3\n",
      "tensor(40)  :  dataset/render2/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee\n",
      "tensor(41)  :  dataset/render2/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6\n",
      "tensor(42)  :  dataset/render2/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647\n",
      "tensor(43)  :  dataset/render2/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83\n",
      "tensor(44)  :  dataset/render2/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a\n",
      "tensor(45)  :  dataset/render2/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40\n",
      "tensor(46)  :  dataset/render2/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34\n",
      "tensor(47)  :  dataset/render2/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15\n",
      "tensor(48)  :  dataset/render3/img/03001627/ffd258571807e6425b1205fcf56bb774\n",
      "tensor(49)  :  dataset/render3/img/03001627/ffd3064cff5757695ecd29875b6f0d44\n",
      "tensor(50)  :  dataset/render3/img/03001627/ffd616229a97642c7ea8c9f2db0a45da\n",
      "tensor(51)  :  dataset/render3/img/03001627/ffd9387a533fe59e251990397636975f\n",
      "tensor(52)  :  dataset/render3/img/03001627/ffdc46ab1cfe759ce6fe3612af521500\n",
      "tensor(53)  :  dataset/render3/img/03001627/ffed7e95160f8edcdea0b1aceafe4876\n",
      "tensor(54)  :  dataset/render3/img/03001627/fff29a99be0df71455a52e01ade8eb6a\n",
      "tensor(55)  :  dataset/render3/img/03001627/fffda9f09223a21118ff2740a556cc3\n",
      "tensor(56)  :  dataset/render3/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee\n",
      "tensor(57)  :  dataset/render3/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6\n",
      "tensor(58)  :  dataset/render3/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647\n",
      "tensor(59)  :  dataset/render3/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83\n",
      "tensor(60)  :  dataset/render3/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a\n",
      "tensor(61)  :  dataset/render3/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40\n",
      "tensor(62)  :  dataset/render3/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34\n",
      "tensor(63)  :  dataset/render3/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15\n",
      "tensor(64)  :  dataset/render4/img/03001627/ffd258571807e6425b1205fcf56bb774\n",
      "tensor(65)  :  dataset/render4/img/03001627/ffd3064cff5757695ecd29875b6f0d44\n",
      "tensor(66)  :  dataset/render4/img/03001627/ffd616229a97642c7ea8c9f2db0a45da\n",
      "tensor(67)  :  dataset/render4/img/03001627/ffd9387a533fe59e251990397636975f\n",
      "tensor(68)  :  dataset/render4/img/03001627/ffdc46ab1cfe759ce6fe3612af521500\n",
      "tensor(69)  :  dataset/render4/img/03001627/ffed7e95160f8edcdea0b1aceafe4876\n",
      "tensor(70)  :  dataset/render4/img/03001627/fff29a99be0df71455a52e01ade8eb6a\n",
      "tensor(71)  :  dataset/render4/img/03001627/fffda9f09223a21118ff2740a556cc3\n",
      "tensor(72)  :  dataset/render4/img/03001627/u1e22cc04-7c4d-4ed5-bda3-8ff8067f22ee\n",
      "tensor(73)  :  dataset/render4/img/03001627/u45c7b89f-d996-4c29-aecf-4b760d1fb2b6\n",
      "tensor(74)  :  dataset/render4/img/03001627/u481ebf18-4bbb-4b49-90c9-7a1e9348b647\n",
      "tensor(75)  :  dataset/render4/img/03001627/u6028f63e-4111-4412-9098-fe5f4f0c7c83\n",
      "tensor(76)  :  dataset/render4/img/03001627/ub5d972a1-de16-4d0a-aa40-85cd3a69aa8a\n",
      "tensor(77)  :  dataset/render4/img/03001627/uca24feec-f0c0-454c-baaf-561530686f40\n",
      "tensor(78)  :  dataset/render4/img/03001627/udf068a6b-e65b-430b-bc17-611b062e2e34\n",
      "tensor(79)  :  dataset/render4/img/03001627/ue639c33f-d415-458c-8ff8-2ef68135af15\n"
     ]
    }
   ],
   "source": [
    "for subdirect in train_dataloader.dataset:\n",
    "    print(subdirect[0], \" : \", label_dict[int(subdirect[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf64c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85b1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer_all = torch.optim.Adam(\n",
    "        [\n",
    "            {\n",
    "                \"params\": decoder.parameters(),\n",
    "                \"lr\": 0.0001,\n",
    "            },\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898a212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée dans l'epoch  0\n",
      "Accès au dossier  dataset/render0/img/03001627/ffd258571807e6425b1205fcf56bb774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m predicted_sdf \u001b[38;5;241m-\u001b[39m real_sdf \u001b[38;5;66;03m# pred_sdf - real_sdf for example\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# calcul gradiant\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m optimizer_all\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/SDF/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SDF/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "from torch.autograd import grad\n",
    "# Define optimization function\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    print(\"Entrée dans l'epoch \", epoch)\n",
    "    \n",
    "    for subdirect in train_dataloader.dataset:\n",
    "        # Get obj id from this path\n",
    "        path = label_dict[int(subdirect[0])]\n",
    "        #obj_id = path.split(\"\\\\\")[3]\n",
    "        \n",
    "        print(\"Accès au dossier \", path)\n",
    "        \n",
    "        # Load each image for this subdirectory\n",
    "        file_list = sorted(os.listdir(path))\n",
    "        list_image = []\n",
    "        for idx, file in enumerate(file_list):\n",
    "            if (file.endswith('.png')):\n",
    "                image = Image.open(os.path.join(path, file))\n",
    "                data = asarray(image)\n",
    "                data = data[:, :, 0]\n",
    "                list_image.append(data)\n",
    "        \n",
    "        # Create a numpy tensor for each image\n",
    "        image_1 = torch.tensor(list_image[0])\n",
    "        image_2 = torch.tensor(list_image[1])\n",
    "        image_3 = torch.tensor(list_image[2])\n",
    "        image_4 = torch.tensor(list_image[3])\n",
    "        image_5 = torch.tensor(list_image[4])\n",
    "        image_6 = torch.tensor(list_image[5])\n",
    "        \n",
    "        # Then concatenate these tensor in one torch.tensor\n",
    "        input_image = torch.stack((image_1, image_2, image_3, image_4, image_5, image_6), 0).float() # concatenate these 6 image so we will have a tensor with shape (1,im_height,im_width,6)\n",
    "        input_image = input_image.unsqueeze(0)\n",
    "        \n",
    "        # Latent vector prediction for these 6 images (len 256)\n",
    "        vect_image = encoder(input_image)\n",
    "        \n",
    "        # Get pos and sdf path for this obj\n",
    "        sdf_obj_path = all_sdfpath_obj[count%len(all_sdfpath_obj)] # todo , maybe we can create a dictionnary (obj_id => sdf_path) insted of list in \"all_sdfpath_obj\"...\n",
    "        count += 1\n",
    "        \n",
    "        # Get the dict or correspondance list for each point and his sdf value\n",
    "        points = np.load(os.path.join(sdf_obj_path, 'pos.npy')) #each point is a array [x,y,z] which represent coordonnées\n",
    "        sdf = np.load(os.path.join(sdf_obj_path, 'sdf.npy'))\n",
    "        \n",
    "        # Predict the sdf of each point\n",
    "        for id in range(0, len(sdf)):\n",
    "            p = points[id]\n",
    "            p = torch.tensor(p)\n",
    "            p = p.unsqueeze(0)\n",
    "            \n",
    "            #print(\"Lecture du point \", id)\n",
    "            \n",
    "            real_sdf = sdf[id]\n",
    "                        \n",
    "            # construct the global latent vector\n",
    "            vect_latent = torch.cat((vect_image,p), 1) #concatenate -> len 259\n",
    "            predicted_sdf = decoder(vect_latent)\n",
    "            \n",
    "            #calcul loss \n",
    "            loss = predicted_sdf - real_sdf # pred_sdf - real_sdf for example\n",
    "            \n",
    "            # calcul gradiant\n",
    "            loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizer_all.step()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b1090e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1c1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
