{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48f5ce8",
   "metadata": {},
   "source": [
    "# Mise en place du réseau de neurone permettant la prédiction de SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84edfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "from torch.autograd import grad\n",
    "from torch.nn.functional import l1_loss\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import tqdm\n",
    "import math\n",
    "import kaolin\n",
    "from kaolin.ops.conversions import marching_tetrahedra\n",
    "from kaolin.metrics.pointcloud import chamfer_distance\n",
    "from kaolin.io.usd import export_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4d244",
   "metadata": {},
   "source": [
    "## 1. Mise en place de l'encodeur, basé sur le réseau ResNet50\n",
    "\n",
    "Le réseau est enrichi d'une couche en entrée permettant de lire un vecteur contenant les informations de 6 images au lieu d'une seule, ainsi que d'une couche de sortie permettant de sortir un vecteur latent de 256 caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22066220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device :  cuda\n"
     ]
    }
   ],
   "source": [
    "# Define encoder\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device : \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c8d70",
   "metadata": {},
   "source": [
    "## 2. Mise en place du décodeur\n",
    "\n",
    "Le décodeur est une suite séquentielle de couches de neurones intercalées avec des couches d'activations qui permettent de prédire une valeur de SDF à partir du vecteur de taille 259 donné en entrée, correspondant au vecteur latent donné par l'encodeur concaténé à la position du point dont on veut prédire le SDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b245c7",
   "metadata": {},
   "source": [
    "##### (Lancer seulement si besoin d'entraînement : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a713b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        #define encoder\n",
    "        model_conv = models.resnet50(pretrained=True)\n",
    "        model_conv.to(device)\n",
    "        \n",
    "        for param in model_conv.parameters():\n",
    "            param.requires_grad = False\n",
    "            param.to(device)\n",
    "            \n",
    "        num_ftrs = model_conv.fc.in_features\n",
    "        model_conv.fc = nn.Linear(num_ftrs, 256)    #Add a new last fully connected layer that will output a 256-features vector\n",
    "        model_conv.to(device)\n",
    "        \n",
    "        encoder = nn.Sequential(\n",
    "        nn.Conv2d(6,3,7,2,3,bias=False),   #(in_channels=6,out_channels=3,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "        model_conv)\n",
    "        \n",
    "        self.add_module(\"encoder\",encoder)\n",
    "\n",
    "        self.layers = [\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        #Add the last layer that will output from 512 to 1 feature\n",
    "        self.layers.append(nn.Linear(self.hidden_size, self.output_size))\n",
    "\n",
    "        #Compile all the layers into a sequential model\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    #Computation performed at every call\n",
    "    def forward(self, x, pt_list):\n",
    "        vect_img = self.encoder(x)\n",
    "        vect_img = vect_img.squeeze()\n",
    "        #creer vecteur les vecteurs latents pour l liste de point\n",
    "        list_vect_lat = []\n",
    "        for p in pt_list:\n",
    "            #print(\"forward info\")\n",
    "            vect_lat = torch.cat((vect_img,p),0)\n",
    "            #print(vect_lat.shape)\n",
    "            list_vect_lat.append(vect_lat)\n",
    "        list_vect_lat = torch.stack(list_vect_lat).float()\n",
    "        #list_vect_lat = list_vect_lat.to(float).\n",
    "        #print(vect_img.type)\n",
    "        #print(p.type)\n",
    "        #print(vect_lat.type)\n",
    "        #print(list_vect_lat.type)\n",
    "        sdf_xyz_pts = torch.tanh(self.layers(list_vect_lat))\n",
    "        return sdf_xyz_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c948b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(6, 3, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=259, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input_size=259 (256 features + 3 positions), hidden_size=512?, output_size=1 (sdf?), num_layers=8\n",
    "network = Net(259, 512, 1, 8)\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5d4ca",
   "metadata": {},
   "source": [
    "## 3. Préparation des données et méthodes liées à l'entraînement\n",
    "\n",
    "###### (Lancer si besoin d'entraîner seulement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0def16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1bcec47c5dc259ea95ca4adb70946a21': 'new_sdf_dataset/1bcec47c5dc259ea95ca4adb70946a21'}\n",
      "{0: 'new_dataset/render0/img/03001627/1bcec47c5dc259ea95ca4adb70946a21', 1: 'new_dataset/render1/img/03001627/1bcec47c5dc259ea95ca4adb70946a21', 2: 'new_dataset/render2/img/03001627/1bcec47c5dc259ea95ca4adb70946a21'}\n"
     ]
    }
   ],
   "source": [
    "#Load data for training\n",
    "\n",
    "# Dictionary that maps integer value to its path value (string)\n",
    "int_labels = []\n",
    "label_dict = {}\n",
    "count = 0\n",
    "\n",
    "#Load data for images\n",
    "dataset_folder = \"new_dataset\"\n",
    "\n",
    "file_list = sorted(os.listdir(dataset_folder))\n",
    "for i, file in enumerate(file_list):\n",
    "    file_img = sorted(os.listdir(os.path.join(dataset_folder, file, \"img/03001627\"))) \n",
    "    \n",
    "    for idx, imgFile in enumerate(file_img):\n",
    "        int_labels.append(count)\n",
    "        datapath_img = os.path.join(dataset_folder, file, \"img/03001627\", imgFile)\n",
    "        label_dict[count] = datapath_img\n",
    "        count+=1\n",
    "\n",
    "#Load data for points array and their sdf value\n",
    "dataset_folder = \"new_sdf_dataset\"\n",
    "sdf_dict = {}\n",
    "\n",
    "file_list = sorted(os.listdir(dataset_folder))\n",
    "for idx, file in enumerate(file_list):\n",
    "    sdfpath_obj = os.path.join(dataset_folder, file)\n",
    "    sdf_dict[file] = sdfpath_obj\n",
    "    \n",
    "print(sdf_dict)\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28d7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets preparation - 70/30\n",
    "#train_dataset, val_dataset = random_split(int_labels, [int(len(int_labels)*0.7), int(len(int_labels)*0.3)])\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(int_labels))\n",
    "#val_dataset = TensorDataset(torch.tensor(val_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=60)\n",
    "#val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf64c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple functions for displaying graphs\n",
    "def my_plot(epochs, train_loss, val_loss):\n",
    "    plt.plot(epochs, train_loss, val_loss)\n",
    "    \n",
    "def my_plot(epochs, train_loss):\n",
    "    plt.plot(epochs, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f85b1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c66e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_regularizer_const(mesh_verts, mesh_faces):\n",
    "    term = torch.zeros_like(mesh_verts)\n",
    "    norm = torch.zeros_like(mesh_verts[..., 0:1])\n",
    "\n",
    "    v0 = mesh_verts[mesh_faces[:, 0], :]\n",
    "    v1 = mesh_verts[mesh_faces[:, 1], :]\n",
    "    v2 = mesh_verts[mesh_faces[:, 2], :]\n",
    "\n",
    "    term.scatter_add_(0, mesh_faces[:, 0:1].repeat(1,3), (v1 - v0) + (v2 - v0))\n",
    "    term.scatter_add_(0, mesh_faces[:, 1:2].repeat(1,3), (v0 - v1) + (v2 - v1))\n",
    "    term.scatter_add_(0, mesh_faces[:, 2:3].repeat(1,3), (v0 - v2) + (v1 - v2))\n",
    "\n",
    "    two = torch.ones_like(v0) * 2.0\n",
    "    norm.scatter_add_(0, mesh_faces[:, 0:1], two)\n",
    "    norm.scatter_add_(0, mesh_faces[:, 1:2], two)\n",
    "    norm.scatter_add_(0, mesh_faces[:, 2:3], two)\n",
    "\n",
    "    term = term / torch.clamp(norm, min=1.0)\n",
    "\n",
    "    return torch.mean(term**2)\n",
    "\n",
    "def loss_f(mesh_verts, mesh_faces, points, it):\n",
    "    laplacian_weight = 0.1\n",
    "    print(\"shape loss\")\n",
    "    print(mesh_verts.unsqueeze(0).shape)\n",
    "    print(points.shape)\n",
    "    pred_points = kaolin.ops.mesh.sample_points(mesh_verts.unsqueeze(0), mesh_faces, 50000)[0][0]\n",
    "    chamfer = kaolin.metrics.pointcloud.chamfer_distance(pred_points.unsqueeze(0), points).mean()\n",
    "    '''\n",
    "    if it > iterations//2:\n",
    "        lap = laplace_regularizer_const(mesh_verts, mesh_faces)\n",
    "        return chamfer + lap * laplacian_weight\n",
    "    '''\n",
    "    return chamfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c304b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11873, 3])\n"
     ]
    }
   ],
   "source": [
    "GT_mesh = mesh_points = kaolin.io.usd.import_pointclouds('./new_sdf_/GT_model.usd')[0].points.to(device)\n",
    "mesh_points = GT_mesh.vertices.cuda().unsqueeze(0)\n",
    "mesh_points.requires_grad = True\n",
    "print(mesh_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "936ae118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "def training(network, num_epochs, data_train):\n",
    "    train_loss = []\n",
    "    grid_res = 128\n",
    "    tets = torch.tensor(([np.load('samples/{}_tets_{}.npz'.format(grid_res, i))['data'] for i in range(4)]), dtype=torch.long, device='cuda').permute(1,0)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                train_epoch_loss = []\n",
    "                train_epoch_acc = []\n",
    "\n",
    "                network.train()\n",
    "                data_t = next(iter(data_train))\n",
    "                with tqdm.trange(len(data_t[0]), unit=\"subfile\", mininterval=0) as bar:\n",
    "                    bar.set_description(f\"Epoch {epoch} / {num_epochs} - Training\")\n",
    "                    for i in bar:\n",
    "                        # Get obj id from this path\n",
    "                        subdirect = data_t[0][i]\n",
    "                        path = label_dict[int(subdirect.item())]\n",
    "                        #obj_id = path.split(\"/\")[-1]\n",
    "                        obj_id = path.split(\"\\\\\")[3]\n",
    "\n",
    "                        #print(\"Accès au dossier \", path)\n",
    "\n",
    "                        # Load each image for this subdirectory\n",
    "                        list_image = []\n",
    "                        for idx, file in enumerate(os.listdir(path)):\n",
    "                            if (file.endswith('.png')):\n",
    "                                image = Image.open(os.path.join(path, file))\n",
    "                                data = asarray(image)\n",
    "                                data = data[:, :, 3]\n",
    "                                list_image.append(data)\n",
    "\n",
    "                        # Create a numpy tensor for each image\n",
    "                        image_1 = torch.tensor(list_image[0], device='cuda')\n",
    "                        image_2 = torch.tensor(list_image[1], device='cuda')\n",
    "                        image_3 = torch.tensor(list_image[2], device='cuda')\n",
    "                        image_4 = torch.tensor(list_image[3], device='cuda')\n",
    "                        image_5 = torch.tensor(list_image[4], device='cuda')\n",
    "                        image_6 = torch.tensor(list_image[5], device='cuda')\n",
    "\n",
    "                        # Then concatenate these tensor in one torch.tensor\n",
    "                        input_image = torch.stack((image_1, image_2, image_3, image_4, image_5, image_6), 0).float() # concatenate these 6 image so we will have a tensor with shape (1,im_height,im_width,6)\n",
    "                        input_image = input_image.unsqueeze(0)\n",
    "                        input_image.to(device)\n",
    "                        input_image.cuda()\n",
    "\n",
    "                        # Latent vector prediction for these 6 images (len 256)\n",
    "                        #vect_image = encoder(input_image)\n",
    "\n",
    "                        # Get pos and sdf path for this obj\n",
    "                        sdf_obj_path = sdf_dict[obj_id] # maybe we can create a dictionnary (obj_id => sdf_path) insted of list in \"all_sdfpath_obj\"...\n",
    "\n",
    "                        # Get the dict or correspondance list for each point and his sdf value\n",
    "                        points = np.load(os.path.join(sdf_obj_path, 'pos.npy')) #each point is a array [x,y,z] which represent coordonnées\n",
    "                        sdf = np.load(os.path.join(sdf_obj_path, 'sdf.npy'))\n",
    "\n",
    "                        # Predict the sdf of each point\n",
    "                        points = torch.tensor(points).to(float).to(device)\n",
    "                        sdf = torch.tensor(sdf).to(float).to(device)\n",
    "                        #print(points.shape)\n",
    "                        #print(sdf.shape)\n",
    "                        points_ds = TensorDataset(points, sdf)\n",
    "                        points_dl = DataLoader(points_ds, shuffle=False, batch_size=1000)\n",
    "                        count = 0\n",
    "                        list_predicted_sdf = []\n",
    "                        total_loss = 0\n",
    "                        \n",
    "                        for pts, real_sdf in points_dl:\n",
    "                            predicted_sdf = network(input_image, pts)\n",
    "                            \n",
    "                            predicted_sdf = predicted_sdf[...,0]\n",
    "                            list_predicted_sdf.append(predicted_sdf)\n",
    "                            \n",
    "                            #print(predicted_sdf.shape)\n",
    "                            #print()\n",
    "                            #print(predicted_sdf)\n",
    "                            \n",
    "                            # Calcul loss and accuracy\n",
    "                            loss = l1_loss((100*predicted_sdf), (100*real_sdf)) # pred_sdf - real_sdf for example\n",
    "                            total_loss += loss\n",
    "                            \n",
    "                            # Calcul gradiant\n",
    "                            loss.backward()\n",
    "                            count +=1\n",
    "                            \n",
    "                            if (count % 100 == 0):\n",
    "                                print(\"loss ;: \", loss)\n",
    "                                \n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                        tet_verts = points.float()\n",
    "                        tet_verts = tet_verts.unsqueeze(0)\n",
    "                        #print(tet_verts.shape)\n",
    "                        #print(tet_verts.type)\n",
    "                        print()\n",
    "                        \n",
    "                        #sdf = torch.cat(for i in list_predicted_sdf)\n",
    "                        #print(sdf.shape)\n",
    "                        #print()\n",
    "                        \n",
    "                        list_predicted_sdf = torch.cat(list_predicted_sdf)\n",
    "                        #sdf = torch.tensor(sdf, dtype=torch.float, device='cuda')\n",
    "                        sdf_pred = list_predicted_sdf.cuda()\n",
    "                        sdf_pred = sdf.unsqueeze(0)\n",
    "                        #print(sdf.shape)\n",
    "                        \n",
    "                        if(epoch>1):\n",
    "                            verts_list, faces_list = marching_tetrahedra(tet_verts, tets, sdf_pred, False)\n",
    "                            stage = export_mesh('./pred_model_'+ str(epoch)+ str(i)+'.usd', vertices=verts_list[0], faces=faces_list[0])\n",
    "                        '''\n",
    "                        mesh_verts, mesh_faces = verts_list[0], faces_list[0]\n",
    "                        mesh_verts.require_grad = True\n",
    "                        #print(mesh_verts)\n",
    "                        loss = loss_f(mesh_verts.float(), mesh_faces, mesh_points.float(), epoch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        print()\n",
    "                        '''\n",
    "                    #train_loss.append(sum(train_epoch_loss)/len(train_epoch_loss))\n",
    "                    #bar.set_postfix(loss=float(sum(train_epoch_loss)/len(train_epoch_loss)), acc=f\"{float(sum(train_epoch_acc)/len(train_epoch_acc))*100:.2f}%\")\n",
    "            \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a400f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 3 - Training:   0%|          | 0/3 [00:00<?, ?subfile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ;:  tensor(10.8964, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7373, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 / 3 - Training:  33%|███▎      | 1/3 [00:35<01:10, 35.31s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss ;:  tensor(10.9863, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7462, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 / 3 - Training:  67%|██████▋   | 2/3 [01:10<00:35, 35.14s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss ;:  tensor(10.9975, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7454, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 3 - Training: 100%|██████████| 3/3 [01:45<00:00, 35.14s/subfile]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 3 - Training:   0%|          | 0/3 [00:00<?, ?subfile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ;:  tensor(10.9866, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7433, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2 / 3 - Training:  33%|███▎      | 1/3 [00:35<01:10, 35.29s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss ;:  tensor(11.0136, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7359, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2 / 3 - Training:  67%|██████▋   | 2/3 [01:10<00:35, 35.25s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss ;:  tensor(10.9993, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7404, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 3 - Training: 100%|██████████| 3/3 [01:47<00:00, 35.88s/subfile]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 3 - Training:   0%|          | 0/3 [00:00<?, ?subfile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ;:  tensor(10.9912, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7294, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 3 / 3 - Training:  33%|███▎      | 1/3 [00:37<01:14, 37.19s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss ;:  tensor(11.0216, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7011, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 3 / 3 - Training:  67%|██████▋   | 2/3 [01:14<00:37, 37.53s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss ;:  tensor(10.9757, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "loss ;:  tensor(10.7410, device='cuda:0', dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 3 - Training: 100%|██████████| 3/3 [01:51<00:00, 37.30s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = training(network, 3, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d411ad6",
   "metadata": {},
   "source": [
    "###### (Lancer seulement si besoin de sauvegarder le modèle - Décommenter / Faire attention à ne pas écraser un modèle déjà existant : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9effc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "#filename = 'finalized_model.sav'\n",
    "pickle.dump(decoder, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1efe2d",
   "metadata": {},
   "source": [
    "## 4. Affichage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77226cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_plot(np.linspace(1, 30, 30).astype(int), train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a5141",
   "metadata": {},
   "source": [
    "## 5. Prédiction de données à partir de l'encodeur et d'un décodeur entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, data):\n",
    "    data_t = next(iter(data))\n",
    "    sdf_predict = []\n",
    "    with tqdm.trange(len(data_t[0]), unit=\"subfile\", mininterval=0) as bar:\n",
    "        bar.set_description(f\"Prédiction du modèle\")\n",
    "        for i in bar:\n",
    "            # Get obj id from this path\n",
    "            subdirect = data_t[0][i]\n",
    "            path = label_dict[int(subdirect.item())]\n",
    "            obj_id = path.split(\"\\\\\")[3]\n",
    "\n",
    "            print(\"Accès au dossier \", path)\n",
    "\n",
    "            # Load each image for this subdirectory\n",
    "            list_image = []\n",
    "            for idx, file in enumerate(os.listdir(path)):\n",
    "                if (file.endswith('.png')):\n",
    "                    image = Image.open(os.path.join(path, file))\n",
    "                    data = asarray(image)\n",
    "                    data = data[:, :, 3]\n",
    "                    list_image.append(data)\n",
    "\n",
    "            # Create a numpy tensor for each image\n",
    "            image_1 = torch.tensor(list_image[0], device='cuda')\n",
    "            image_2 = torch.tensor(list_image[1], device='cuda')\n",
    "            image_3 = torch.tensor(list_image[2], device='cuda')\n",
    "            image_4 = torch.tensor(list_image[3], device='cuda')\n",
    "            image_5 = torch.tensor(list_image[4], device='cuda')\n",
    "            image_6 = torch.tensor(list_image[5], device='cuda')\n",
    "\n",
    "            # Then concatenate these tensor in one torch.tensor\n",
    "            input_image = torch.stack((image_1, image_2, image_3, image_4, image_5, image_6), 0).float() # concatenate these 6 image so we will have a tensor with shape (1,im_height,im_width,6)\n",
    "            input_image = input_image.unsqueeze(0)\n",
    "            input_image.to(device)\n",
    "            input_image.cuda()\n",
    "\n",
    "            # Latent vector prediction for these 6 images (len 256)\n",
    "            vect_image = encoder(input_image)\n",
    "\n",
    "            # Get pos and sdf path for this obj\n",
    "            sdf_obj_path = sdf_dict[obj_id] # maybe we can create a dictionnary (obj_id => sdf_path) insted of list in \"all_sdfpath_obj\"...\n",
    "\n",
    "            # Get the dict or correspondance list for each point and his sdf value\n",
    "            points = np.load(os.path.join(sdf_obj_path, 'pos.npy')) #each point is a array [x,y,z] which represent coordonnées\n",
    "            sdf = np.load(os.path.join(sdf_obj_path, 'sdf.npy'))\n",
    "            \n",
    "            list_vect_latent = []\n",
    "            \n",
    "            # Predict the sdf of each point\n",
    "            for id in range(0, len(sdf)):\n",
    "                p = points[id]\n",
    "                p = torch.tensor(p, device='cuda')\n",
    "                p = p.unsqueeze(0)\n",
    "\n",
    "                #print(\"Lecture du point \", sdf_obj_path)\n",
    "\n",
    "                # Construct the global latent vector\n",
    "                vect_latent = torch.cat((vect_image,p), 1) #concatenate -> len 259\n",
    "                vect_latent = vect_latent.squeeze()\n",
    "                vect_latent.to(device)\n",
    "                vect_latent.cuda()\n",
    "\n",
    "                list_vect_latent.append(vect_latent)\n",
    "\n",
    "            stack_vect = torch.stack(list_vect_latent)\n",
    "            print(\"Stack_vect :\", stack_vect.shape)\n",
    "            nb_loops = 277410//200\n",
    "            predicted_sdf_list = []\n",
    "            \n",
    "            for i in range(0, nb_loops):\n",
    "                low_threshold = 200*i\n",
    "                upper_threshold = 200*(i+1)\n",
    "                tmp_vect = stack_vect[low_threshold :upper_threshold, :]\n",
    "                \n",
    "                predicted_sdf = decoder(tmp_vect)\n",
    "                \n",
    "                predicted_sdf_list.append(predicted_sdf)\n",
    "            \n",
    "            low_threshold = 277410 - (277410%200)\n",
    "            tmp_vect = stack_vect[low_threshold :277410, :]\n",
    "            predicted_sdf = decoder(tmp_vect)\n",
    "            predicted_sdf_list.append(predicted_sdf)\n",
    "            \n",
    "            all_predicted_sdf = torch.cat(predicted_sdf_list, dim=0)\n",
    "            print(all_predicted_sdf.shape)\n",
    "            \n",
    "    return all_predicted_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa90608",
   "metadata": {},
   "source": [
    "###### (Lancer seulement si besoin de charger un modèle du décodeur : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2f2aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=259, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# load the model from disk\n",
    "filename = 'finalized_model.sav'\n",
    "decoder = pickle.load(open(filename, 'rb'))\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1830b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data for predicting\n",
    "\n",
    "# Dictionary that maps integer value to its path value (string)\n",
    "int_labels = []\n",
    "label_dict = {}\n",
    "count = 0\n",
    "\n",
    "#Load data for images\n",
    "dataset_folder = \"predict\"\n",
    "\n",
    "file_list = sorted(os.listdir(dataset_folder))\n",
    "for i, file in enumerate(file_list):\n",
    "    file_img = sorted(os.listdir(os.path.join(dataset_folder, file, \"img/03001627\"))) \n",
    "    \n",
    "    for idx, imgFile in enumerate(file_img):\n",
    "        int_labels.append(count)\n",
    "        datapath_img = os.path.join(dataset_folder, file, \"img/03001627\", imgFile)\n",
    "        label_dict[count] = datapath_img\n",
    "        count+=1\n",
    "\n",
    "#Load data for points array and their sdf value\n",
    "dataset_folder = \"sdf_predict\"\n",
    "sdf_dict = {}\n",
    "\n",
    "file_list = sorted(os.listdir(dataset_folder))\n",
    "for idx, file in enumerate(file_list):\n",
    "    sdfpath_obj = os.path.join(dataset_folder, file)\n",
    "    sdf_dict[file] = sdfpath_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7b4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset preparation\n",
    "dataset = TensorDataset(torch.tensor(int_labels))\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ce6b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prédiction du modèle:   0%|                                                                 | 0/1 [00:00<?, ?subfile/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accès au dossier  predict\\render0\\img/03001627\\1be38f2624022098f71e06115e9c3b3e\n",
      "Stack_vect : torch.Size([277410, 259])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prédiction du modèle: 100%|█████████████████████████████████████████████████████████| 1/1 [00:56<00:00, 56.41s/subfile]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277400\n",
      "torch.Size([277410, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = predict(encoder, decoder, dataloader)\n",
    "sdf = sdf[...,0].unsqueeze(0)\n",
    "print(\"SDF final :\", sdf.shape)\n",
    "\n",
    "sdf = sdf.numpy()\n",
    "#save('/sdf_predict/sdf.npy', sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798b39a",
   "metadata": {},
   "source": [
    "## 6. Reconstruction d'un modèle 3D à partir de valeurs de SDF prédites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd73e30",
   "metadata": {},
   "source": [
    "###### (Lancer seulement si besoin de charger un fichier de valeur SDF - changer le nom du fichier à charger : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15392929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7324503  0.72935754 0.71682054 0.73338735 0.7207073  0.7202037\n",
      " 0.71025324 0.7186455  0.71252084 0.70285153 0.7030958  0.7040121\n",
      " 0.6896396  0.69207627 0.6857143  0.67638373 0.6762767  0.67758965\n",
      " 0.4920022  0.48254684]\n",
      "torch.Size([1, 277410])\n"
     ]
    }
   ],
   "source": [
    "sdf = np.load('sdf_dataset_old/1bcec47c5dc259ea95ca4adb70946a21/sdf.npy')\n",
    "print(sdf[:20])\n",
    "\n",
    "sdf = torch.tensor(sdf, dtype=torch.float, device='cuda')\n",
    "sdf = sdf.unsqueeze(0)\n",
    "\n",
    "print(sdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06bf3e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tet_verts :  torch.Size([1, 277410, 3])\n",
      "Tets :  torch.Size([1524684, 4])\n",
      "SDF :  torch.Size([1, 277410])\n",
      "Vertices mesh  torch.Size([11873, 3])\n",
      "Faces mesh :  torch.Size([23702, 3])\n"
     ]
    }
   ],
   "source": [
    "#Using MarchingTetrahedra to get the mesh\n",
    "grid_res = 128\n",
    "tet_verts = torch.tensor(np.load('samples/{}_verts.npz'.format(grid_res))['data'], dtype=torch.float, device='cuda')\n",
    "tets = torch.tensor(([np.load('samples/{}_tets_{}.npz'.format(grid_res, i))['data'] for i in range(4)]), dtype=torch.long, device='cuda').permute(1,0)\n",
    "\n",
    "tet_verts = tet_verts.to(float)\n",
    "#y_values = tet_verts[..., 1]\n",
    "#z_values = tet_verts[..., 2]\n",
    "#tet_verts[...,1] = z_values\n",
    "#tet_verts[...,2] = y_values\n",
    "tet_verts = tet_verts.unsqueeze(0)\n",
    "\n",
    "print(\"Tet_verts : \", tet_verts.shape)\n",
    "print(\"Tets : \", tets.shape)\n",
    "print(\"SDF : \", sdf.shape)\n",
    "\n",
    "verts_list, faces_list = marching_tetrahedra(tet_verts, tets, sdf, False)\n",
    "print(\"Vertices mesh \", verts_list[0].shape)\n",
    "print(\"Faces mesh : \", faces_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c844768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save mesh to .usd file\n",
    "from kaolin.io.usd import export_mesh\n",
    "stage = export_mesh('./GT_model_1.usd', vertices=verts_list[0], faces=faces_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce6e27",
   "metadata": {},
   "source": [
    "# ANNEXES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659281e7",
   "metadata": {},
   "source": [
    "## 1. Librairie mesh_to_sdf permettant de calculer des valeurs de sdf à partir d'un modèle et d'une liste de points\n",
    "\n",
    "###### (Lancer seulement si besoin de calculer des valeurs réelles de sdf pour un modèle en .obj : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba73b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device :  cpu\n",
      "277410\n"
     ]
    }
   ],
   "source": [
    "import mesh_to_sdf\n",
    "import trimesh\n",
    "import pyrender\n",
    "import numpy as np\n",
    "import argparse, sys, os\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device : \", device)\n",
    "\n",
    "grid_res = 128\n",
    "tet_verts = torch.tensor(np.load('samples/{}_verts.npz'.format(grid_res))['data'], dtype=torch.float, device=device)\n",
    "\n",
    "mesh = trimesh.load('models/03001627/ff2223a085d32243696b74614952b2d0/model.obj')\n",
    "\n",
    "tet_verts = tet_verts.numpy()\n",
    "sdf = mesh_to_sdf.mesh_to_sdf(mesh, tet_verts, surface_point_method='scan', sign_method='normal', bounding_radius=None, scan_count=100, scan_resolution=400, sample_point_count=10000000, normal_sample_count=11)\n",
    "\n",
    "print(len(sdf))\n",
    "save('sdf_model7.npy', sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014d486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5     0.5     0.4844]\n",
      " [ 0.4844  0.5     0.4922]\n",
      " [ 0.4922  0.4844  0.4844]\n",
      " [ 0.5     0.4922  0.5   ]\n",
      " [ 0.4844  0.4844  0.5   ]\n",
      " [ 0.5     0.4844  0.4844]\n",
      " [ 0.4766  0.4844  0.4844]\n",
      " [ 0.4688  0.4922  0.5   ]\n",
      " [ 0.4688  0.5     0.4844]\n",
      " [ 0.4531  0.5     0.4922]\n",
      " [ 0.461   0.4844  0.4844]\n",
      " [ 0.4531  0.4844  0.5   ]\n",
      " [ 0.4453  0.4844  0.4844]\n",
      " [ 0.4375  0.4922  0.5   ]\n",
      " [ 0.4375  0.5     0.4844]\n",
      " [ 0.4219  0.5     0.4922]\n",
      " [ 0.4297  0.4844  0.4844]\n",
      " [ 0.4219  0.4844  0.5   ]\n",
      " [-0.5    -0.4766 -0.5   ]\n",
      " [-0.5    -0.4844 -0.4844]]\n",
      "(277410, 3)\n"
     ]
    }
   ],
   "source": [
    "from numpy import save\n",
    "grid_res = 128\n",
    "points = np.load('samples/{}_verts.npz'.format(grid_res))['data']\n",
    "print(points[:20])\n",
    "\n",
    "print(points.shape)\n",
    "save('pos.npy', points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
